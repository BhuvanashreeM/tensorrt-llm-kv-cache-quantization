{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcbw62tIF4LI"
   },
   "source": [
    "# KV Cache Quantisation with TensorRT-LLM on a Single A100 GPU\n",
    "\n",
    "This notebook demonstrates how I set up a **self-contained TensorRT-LLM workflow** on a single A100 GPU.  \n",
    "I show baseline FP16 inference and then progressively apply **KV-cache quantisation** strategies to optimise memory usage and speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlN4MhLOF7Tv"
   },
   "source": [
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "Clone the TensorRT-LLM repository, install dependencies, and prepare CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TensorRT-LLM'...\n",
      "remote: Enumerating objects: 135576, done.\u001b[K\n",
      "remote: Counting objects: 100% (319/319), done.\u001b[K\n",
      "remote: Compressing objects: 100% (178/178), done.\u001b[K\n",
      "remote: Total 135576 (delta 207), reused 141 (delta 141), pack-reused 135257 (from 2)\u001b[K\n",
      "Receiving objects: 100% (135576/135576), 1.59 GiB | 25.90 MiB/s, done.\n",
      "Resolving deltas: 100% (88806/88806), done.\n",
      "Updating files: 100% (6623/6623), done.\n",
      "Filtering content: 100% (2668/2668), 1.59 GiB | 20.63 MiB/s, done.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting tensorrt_llm\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-llm/tensorrt_llm-1.2.0rc0-cp312-cp312-linux_x86_64.whl (3724.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 GB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (1.10.1)\n",
      "Requirement already satisfied: build in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (1.3.0)\n",
      "Collecting colored (from tensorrt_llm)\n",
      "  Downloading colored-2.3.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: cuda-python<13,>=12 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (12.6.2.post1)\n",
      "Requirement already satisfied: diffusers>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.35.1)\n",
      "Requirement already satisfied: lark in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (1.3.0)\n",
      "Collecting mpi4py (from tensorrt_llm)\n",
      "  Downloading mpi4py-4.1.0-cp312-cp312-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (16 kB)\n",
      "Collecting numpy<2 (from tensorrt_llm)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting onnx>=1.18.0 (from tensorrt_llm)\n",
      "  Downloading onnx-1.19.1rc1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting onnx_graphsurgeon>=0.5.2 (from tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/onnx-graphsurgeon/onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (1.109.1)\n",
      "Collecting polygraphy (from tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/polygraphy/polygraphy-0.49.26-py2.py3-none-any.whl (372 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.8/372.8 kB\u001b[0m \u001b[31m164.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (5.9.5)\n",
      "Requirement already satisfied: nvidia-ml-py<13,>=12 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (12.575.51)\n",
      "Collecting pulp (from tensorrt_llm)\n",
      "  Downloading pulp-3.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (2.2.2)\n",
      "Collecting h5py==3.12.1 (from tensorrt_llm)\n",
      "  Downloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting StrEnum (from tensorrt_llm)\n",
      "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.2.1)\n",
      "Collecting tensorrt~=10.11.0 (from tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt/tensorrt-10.11.0.33.tar.gz (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting torch<=2.8.0a0,>=2.7.1 (from tensorrt_llm)\n",
      "  Downloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.23.0+cu126)\n",
      "Collecting nvidia-modelopt~=0.33.0 (from nvidia-modelopt[torch]~=0.33.0->tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-modelopt/nvidia_modelopt-0.33.1-py3-none-manylinux_2_28_x86_64.whl (751 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.7/751.7 kB\u001b[0m \u001b[31m206.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (2.27.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (12.6.77)\n",
      "Collecting transformers==4.56.0 (from tensorrt_llm)\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: prometheus_client in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.23.1)\n",
      "Collecting prometheus_fastapi_instrumentator (from tensorrt_llm)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pydantic>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (2.11.9)\n",
      "Requirement already satisfied: pydantic-settings[yaml] in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (2.11.0)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (2.3.0)\n",
      "Collecting pillow==10.3.0 (from tensorrt_llm)\n",
      "  Downloading pillow-10.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: wheel<=0.45.1 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.45.1)\n",
      "Collecting optimum (from tensorrt_llm)\n",
      "  Downloading optimum-1.27.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting datasets==3.1.0 (from tensorrt_llm)\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting evaluate (from tensorrt_llm)\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: mpmath>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (1.3.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (8.3.0)\n",
      "Collecting click_option_group (from tensorrt_llm)\n",
      "  Downloading click_option_group-0.5.8-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting aenum (from tensorrt_llm)\n",
      "  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (26.2.1)\n",
      "Collecting fastapi<=0.117.1,>=0.115.4 (from tensorrt_llm)\n",
      "  Downloading fastapi-0.117.1-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.37.0)\n",
      "Requirement already satisfied: setuptools<80 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (75.2.0)\n",
      "Collecting ordered-set (from tensorrt_llm)\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.17.1)\n",
      "Collecting patchelf==0.18.0 (from tensorrt_llm)\n",
      "  Downloading patchelf-0.18.0.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.musllinux_1_1_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.8.1)\n",
      "Collecting flashinfer-python>=0.3.0 (from tensorrt_llm)\n",
      "  Downloading flashinfer_python-0.4.0rc4.tar.gz (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[33m  WARNING: Requested flashinfer-python>=0.3.0 from https://files.pythonhosted.org/packages/94/ec/bdcc0ec502994d544cbe69763d999458ae2deda67e58c1cb2d85867677c4/flashinfer_python-0.4.0rc4.tar.gz (from tensorrt_llm), but installing version 0.4.0rc4\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (4.12.0.88)\n",
      "Collecting xgrammar==0.1.25 (from tensorrt_llm)\n",
      "  Downloading xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting llguidance==0.7.29 (from tensorrt_llm)\n",
      "  Downloading llguidance-0.7.29-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (4.25.1)\n",
      "Collecting backoff (from tensorrt_llm)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: nvtx in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.2.13)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (3.10.0)\n",
      "Collecting meson (from tensorrt_llm)\n",
      "  Downloading meson-1.9.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ninja (from tensorrt_llm)\n",
      "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting etcd3 (from tensorrt_llm)\n",
      "  Downloading etcd3-0.12.0.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting blake3 (from tensorrt_llm)\n",
      "  Downloading blake3-1.0.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (217 bytes)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.13.1)\n",
      "Collecting triton==3.3.1 (from tensorrt_llm)\n",
      "  Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (0.11.0)\n",
      "Requirement already satisfied: blobfile in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (3.1.0)\n",
      "Collecting openai-harmony==0.0.4 (from tensorrt_llm)\n",
      "  Downloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting nvidia-cutlass-dsl==4.2.1 (from tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cutlass-dsl/nvidia_cutlass_dsl-4.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (62.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=4.25.8 in /usr/local/lib/python3.12/dist-packages (from tensorrt_llm) (5.29.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (0.70.16)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0->tensorrt_llm)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (3.12.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (0.35.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==3.1.0->tensorrt_llm) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from nvidia-cutlass-dsl==4.2.1->tensorrt_llm) (4.15.0)\n",
      "Collecting cuda-python<13,>=12 (from tensorrt_llm)\n",
      "  Downloading cuda_python-12.9.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.0->tensorrt_llm) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.0->tensorrt_llm) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.0->tensorrt_llm) (0.6.2)\n",
      "Collecting cuda-bindings~=12.9.2 (from cuda-python<13,>=12->tensorrt_llm)\n",
      "  Downloading cuda_bindings-12.9.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.27.0->tensorrt_llm) (8.7.0)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<=0.117.1,>=0.115.4->tensorrt_llm) (0.48.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from flashinfer-python>=0.3.0->tensorrt_llm) (0.9.0)\n",
      "Collecting apache-tvm-ffi==0.1.0b11 (from flashinfer-python>=0.3.0->tensorrt_llm)\n",
      "  Downloading apache_tvm_ffi-0.1.0b11-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-cudnn-frontend>=1.13.0 (from flashinfer-python>=0.3.0->tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cudnn-frontend/nvidia_cudnn_frontend-1.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-modelopt-core==0.33.1 (from nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-modelopt-core/nvidia_modelopt_core-0.33.1-cp312-cp312-manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->tensorrt_llm) (13.9.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->tensorrt_llm) (1.16.2)\n",
      "Collecting torchprofile>=0.0.4 (from nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->tensorrt_llm)\n",
      "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
      "\u001b[33mWARNING: nvidia-modelopt 0.33.1 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.18.0->tensorrt_llm) (0.5.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.1->tensorrt_llm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.1->tensorrt_llm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.1->tensorrt_llm) (0.4.2)\n",
      "Collecting tensorrt_cu12==10.11.0.33 (from tensorrt~=10.11.0->tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-cu12/tensorrt_cu12-10.11.0.33.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tensorrt_cu12_libs==10.11.0.33 (from tensorrt_cu12==10.11.0.33->tensorrt~=10.11.0->tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-cu12-libs/tensorrt_cu12_libs-10.11.0.33-py2.py3-none-manylinux_2_28_x86_64.whl (3095.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 GB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorrt_cu12_bindings==10.11.0.33 (from tensorrt_cu12==10.11.0.33->tensorrt~=10.11.0->tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/tensorrt-cu12-bindings/tensorrt_cu12_bindings-10.11.0.33-cp312-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.12/dist-packages (from tensorrt_cu12_libs==10.11.0.33->tensorrt_cu12==10.11.0.33->tensorrt~=10.11.0->tensorrt_llm) (12.6.77)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (12.6.80)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cudnn-cu12/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (12.5.4.2)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-cusparselt-cu12/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12 (from tensorrt_llm)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nccl-cu12/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (1.11.1.6)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.12/dist-packages (from blobfile->tensorrt_llm) (3.23.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from blobfile->tensorrt_llm) (2.5.0)\n",
      "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.12/dist-packages (from blobfile->tensorrt_llm) (5.4.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build->tensorrt_llm) (1.2.0)\n",
      "Requirement already satisfied: grpcio>=1.27.1 in /usr/local/lib/python3.12/dist-packages (from etcd3->tensorrt_llm) (1.75.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from etcd3->tensorrt_llm) (1.17.0)\n",
      "Requirement already satisfied: tenacity>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from etcd3->tensorrt_llm) (8.5.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->tensorrt_llm) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->tensorrt_llm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->tensorrt_llm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->tensorrt_llm) (0.27.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->tensorrt_llm) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->tensorrt_llm) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->tensorrt_llm) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->tensorrt_llm) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->tensorrt_llm) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->tensorrt_llm) (2.9.0.post0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->tensorrt_llm) (4.9.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai->tensorrt_llm) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai->tensorrt_llm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai->tensorrt_llm) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai->tensorrt_llm) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai->tensorrt_llm) (1.3.1)\n",
      "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python-headless (from tensorrt_llm)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->tensorrt_llm) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->tensorrt_llm) (2025.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings[yaml]->tensorrt_llm) (1.1.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile->tensorrt_llm) (2.0.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from tensorrt_llm)\n",
      "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->tensorrt_llm) (0.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai->tensorrt_llm) (3.10)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile->tensorrt_llm) (2.23)\n",
      "Collecting cuda-pathfinder~=1.1 (from cuda-bindings~=12.9.2->cuda-python<13,>=12->tensorrt_llm)\n",
      "  Downloading cuda_pathfinder-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.1.0->tensorrt_llm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.1.0->tensorrt_llm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.1.0->tensorrt_llm) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.1.0->tensorrt_llm) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.1.0->tensorrt_llm) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.1.0->tensorrt_llm) (1.20.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai->tensorrt_llm) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai->tensorrt_llm) (1.0.9)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.0->datasets==3.1.0->tensorrt_llm) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.1.0->tensorrt_llm) (3.4.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers>=0.27.0->tensorrt_llm) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.8.0a0,>=2.7.1->tensorrt_llm) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->tensorrt_llm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->tensorrt_llm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->nvidia-modelopt~=0.33.0->nvidia-modelopt[torch]~=0.33.0->tensorrt_llm) (0.1.2)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'patchelf' candidate (version 0.18.0.0 at https://files.pythonhosted.org/packages/69/85/0582468b77c57e2e9cfbfcd4faeaec39741dc56898f069c43baac3fcb55a/patchelf-0.18.0.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.musllinux_1_1_x86_64.whl (from https://pypi.org/simple/patchelf/))\n",
      "Reason for being yanked: https://github.com/mayeut/patchelf-pypi/issues/87\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llguidance-0.7.29-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading patchelf-0.18.0.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.musllinux_1_1_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cuda_python-12.9.2-py3-none-any.whl (7.6 kB)\n",
      "Downloading fastapi-0.117.1-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading apache_tvm_ffi-0.1.0b11-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.19.1rc1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m506.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aenum-3.1.16-py3-none-any.whl (165 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading blake3-1.0.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (387 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.9/387.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click_option_group-0.5.8-py3-none-any.whl (11 kB)\n",
      "Downloading colored-2.3.1-py3-none-any.whl (19 kB)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading meson-1.9.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpi4py-4.1.0-cp312-cp312-manylinux1_x86_64.manylinux_2_5_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading optimum-1.27.0-py3-none-any.whl (425 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.8/425.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading pulp-3.3.0-py3-none-any.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Downloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cuda_bindings-12.9.2-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
      "Downloading cuda_pathfinder-1.3.0-py3-none-any.whl (27 kB)\n",
      "Building wheels for collected packages: flashinfer-python, tensorrt, tensorrt_cu12, etcd3\n",
      "  Building wheel for flashinfer-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flashinfer-python: filename=flashinfer_python-0.4.0rc4-py3-none-any.whl size=5746594 sha256=70f7227d89893bdc11313a0d2ce930301516a226ab3963e1aa863eaa2eea2e5c\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/a7/1e/26ce70d040d86b1811d2ae4148ea6bcc7b70b96ed48cbf61d1\n",
      "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tensorrt: filename=tensorrt-10.11.0.33-py2.py3-none-any.whl size=46636 sha256=66786cf9fa679a6465e5e9ffecab32ec085a5bd547c35d3e2395f263e089eed0\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/c3/c2/1e80a11ddcebc60a8c3bcde164db5e52dbf74dad28bf98603a\n",
      "  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.11.0.33-py2.py3-none-any.whl size=17480 sha256=047e81c034f41c23f1000733228da306f9e3e056a0cd3b30122baea5ab26f1af\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/d4/e7/4e3c60d575978d9cac11a3f0e59f28419ed4e8a7cfcae388cb\n",
      "  Building wheel for etcd3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for etcd3: filename=etcd3-0.12.0-py2.py3-none-any.whl size=42867 sha256=99c2d117b6371899b5a97b134f323e271cdaae1cd9206ac40f896f77275b81c0\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/ab/cb/8178a773ec0cee5434f923ad304941e794ed6a8392f0cd5f93\n",
      "Successfully built flashinfer-python tensorrt tensorrt_cu12 etcd3\n",
      "Installing collected packages: tensorrt_cu12_bindings, StrEnum, patchelf, nvidia-cusparselt-cu12, aenum, triton, tensorrt_cu12_libs, pulp, polygraphy, pillow, ordered-set, nvidia-nccl-cu12, nvidia-modelopt-core, nvidia-cudnn-frontend, nvidia-cudnn-cu12, numpy, ninja, mpi4py, meson, llguidance, fsspec, cuda-pathfinder, colored, click_option_group, blake3, backoff, apache-tvm-ffi, tensorrt_cu12, opencv-python-headless, h5py, etcd3, cuda-bindings, torch, tensorrt, prometheus_fastapi_instrumentator, openai-harmony, onnx, fastapi, cuda-python, transformers, torchvision, onnx_graphsurgeon, nvidia-cutlass-dsl, datasets, xgrammar, torchprofile, optimum, flashinfer-python, evaluate, nvidia-modelopt, tensorrt_llm\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.3.0\n",
      "    Uninstalling pillow-11.3.0:\n",
      "      Successfully uninstalled pillow-11.3.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.12.0.88\n",
      "    Uninstalling opencv-python-headless-4.12.0.88:\n",
      "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.14.0\n",
      "    Uninstalling h5py-3.14.0:\n",
      "      Successfully uninstalled h5py-3.14.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0+cu126\n",
      "    Uninstalling torch-2.8.0+cu126:\n",
      "      Successfully uninstalled torch-2.8.0+cu126\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.118.0\n",
      "    Uninstalling fastapi-0.118.0:\n",
      "      Successfully uninstalled fastapi-0.118.0\n",
      "  Attempting uninstall: cuda-python\n",
      "    Found existing installation: cuda-python 12.6.2.post1\n",
      "    Uninstalling cuda-python-12.6.2.post1:\n",
      "      Successfully uninstalled cuda-python-12.6.2.post1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.56.2\n",
      "    Uninstalling transformers-4.56.2:\n",
      "      Successfully uninstalled transformers-4.56.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.23.0+cu126\n",
      "    Uninstalling torchvision-0.23.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.23.0+cu126\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.7.1 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed StrEnum-0.4.15 aenum-3.1.16 apache-tvm-ffi-0.1.0b11 backoff-2.2.1 blake3-1.0.7 click_option_group-0.5.8 colored-2.3.1 cuda-bindings-12.9.2 cuda-pathfinder-1.3.0 cuda-python-12.9.2 datasets-3.1.0 etcd3-0.12.0 evaluate-0.4.6 fastapi-0.117.1 flashinfer-python-0.4.0rc4 fsspec-2024.9.0 h5py-3.12.1 llguidance-0.7.29 meson-1.9.1 mpi4py-4.1.0 ninja-1.13.0 numpy-1.26.4 nvidia-cudnn-cu12-9.5.1.17 nvidia-cudnn-frontend-1.14.1 nvidia-cusparselt-cu12-0.6.3 nvidia-cutlass-dsl-4.2.1 nvidia-modelopt-0.33.1 nvidia-modelopt-core-0.33.1 nvidia-nccl-cu12-2.26.2 onnx-1.19.1rc1 onnx_graphsurgeon-0.5.8 openai-harmony-0.0.4 opencv-python-headless-4.11.0.86 optimum-1.27.0 ordered-set-4.1.0 patchelf-0.18.0.0 pillow-10.3.0 polygraphy-0.49.26 prometheus_fastapi_instrumentator-7.1.0 pulp-3.3.0 tensorrt-10.11.0.33 tensorrt_cu12-10.11.0.33 tensorrt_cu12_bindings-10.11.0.33 tensorrt_cu12_libs-10.11.0.33 tensorrt_llm-1.2.0rc0 torch-2.7.1 torchprofile-0.0.4 torchvision-0.22.1 transformers-4.56.0 triton-3.3.1 xgrammar-0.1.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "821249cbc4a945fab1a7ed7655dbe2ce",
       "pip_warning": {
        "packages": [
         "PIL",
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.12/dist-packages (12.0.0)\n",
      "Requirement already satisfied: mpi4py in /usr/local/lib/python3.12/dist-packages (4.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pynvml) (12.575.51)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/TensorRT-LLM.git\n",
    "!pip install tensorrt_llm -U --pre --extra-index-url https://pypi.nvidia.com\n",
    "!pip install huggingface_hub pynvml mpi4py\n",
    "!pip install -r TensorRT-LLM/examples/models/core/llama/requirements.txt\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "print(\"CUDA_HOME =\", os.environ[\"CUDA_HOME\"])\n",
    "\n",
    "# Install specific versions of cuda-python and nvidia-cudnn-cu12 for compatibility\n",
    "!pip install --upgrade --force-reinstall cuda-python==12.2.1\n",
    "!pip install nvidia-cudnn-cu12==8.9.2.26\n",
    "\n",
    "print(\"CUDA_HOME =\", os.environ[\"CUDA_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall cuda-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMlEkIJcGBe-"
   },
   "source": [
    "## 2. Download Model from Hugging Face\n",
    "\n",
    "I use the Hugging Face Hub to fetch **Llama-3.2-1B**.  \n",
    "The model is stored locally for conversion into TensorRT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8e6752b7ad4a1193acacb70d4140c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be137ebfc0d344a3b55a647a1f583cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE.txt:   0%|          | 0.00/7.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacbf1e88dfa4eb09ae900ab22db927a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "USE_POLICY.md:   0%|          | 0.00/6.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39e0a25740a4652b0cfce6a7ab7ecbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/41.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04eb450e515c4823a23f67be236e847a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8596b3dce12e44879f67f3a6fa89f46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8be2900b12e4bfe94d827a18a5e4346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b8141d788e413d9980bc82a2cf700b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65df77a56cde4bd4be700d97cbaa9667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/consolidated.00.pth:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910d0ed82acc4b2888368076e62bf2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec91b1ef3a348a393f328dbdac6f40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/tokenizer.model:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4061311e5c864eb9a0b08897560c5372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa360d2e145547bd98e0961371bcd1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd2dbdb3eb9475fa4f5f0e32514cb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/tmp/hf_models/meta-llama/Llama-3.2-1B'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    local_dir=\"tmp/hf_models/meta-llama/Llama-3.2-1B\",\n",
    "    max_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pwlaGX3GGdM"
   },
   "source": [
    "## 3. Optimisation Scenario 0: Baseline FP16 Engine\n",
    "\n",
    "First, I convert the HF checkpoint and build a **TensorRT FP16 engine**.  \n",
    "This serves as the baseline for comparison against quantised versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME = /usr/local/cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
    "print(\"CUDA_HOME =\", os.environ[\"CUDA_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/content/./TensorRT-LLM/examples/models/core/llama/convert_checkpoint.py\", line 10, in <module>\n",
      "    import tensorrt_llm\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt_llm/__init__.py\", line 66, in <module>\n",
      "    import tensorrt_llm._torch.models as torch_models\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt_llm/_torch/__init__.py\", line 1, in <module>\n",
      "    from .llm import LLM\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt_llm/_torch/llm.py\", line 1, in <module>\n",
      "    from tensorrt_llm.llmapi.llm import _TorchLLM\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt_llm/llmapi/__init__.py\", line 1, in <module>\n",
      "    from ..disaggregated_params import DisaggregatedParams\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt_llm/disaggregated_params.py\", line 11, in <module>\n",
      "    from tensorrt_llm.bindings import executor as tllme\n",
      "ImportError: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "Building FP16 Engine...\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/trtllm-build\", line 5, in <module>\n",
      "    from tensorrt_llm.commands.build import main\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt_llm/__init__.py\", line 66, in <module>\n",
      "    import tensorrt_llm._torch.models as torch_models\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt_llm/_torch/__init__.py\", line 1, in <module>\n",
      "    from .llm import LLM\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt_llm/_torch/llm.py\", line 1, in <module>\n",
      "    from tensorrt_llm.llmapi.llm import _TorchLLM\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt_llm/llmapi/__init__.py\", line 1, in <module>\n",
      "    from ..disaggregated_params import DisaggregatedParams\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tensorrt_llm/disaggregated_params.py\", line 11, in <module>\n",
      "    from tensorrt_llm.bindings import executor as tllme\n",
      "ImportError: libcuda.so.1: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python ./TensorRT-LLM/examples/models/core/llama/convert_checkpoint.py \\\n",
    "  --model_dir ./tmp/hf_models/meta-llama/Llama-3.2-1B \\\n",
    "  --output_dir ./tmp/trt_engines/1-gpu/ \\\n",
    "  --dtype float16\n",
    "\n",
    "print(\"Building FP16 Engine...\")\n",
    "!trtllm-build --checkpoint_dir ./tmp/trt_engines/1-gpu/ \\\n",
    "              --output_dir ./tmp/trt_engines/llama_fp16 \\\n",
    "              --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_G0hV8v1GMSX"
   },
   "source": [
    "## 4. Optimisation Scenario 1: INT8 KV Cache Only\n",
    "\n",
    "Here, I quantise the **KV-cache to INT8**, while leaving weights in FP16.  \n",
    "This reduces memory bandwidth usage during long sequence decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./tmp/trt_engines/1-gpu-int8-ckpt\n",
    "\n",
    "!python ./TensorRT-LLM/examples/models/core/llama/convert_checkpoint.py \\\n",
    "  --model_dir ./tmp/hf_models/meta-llama/Llama-3.2-1B \\\n",
    "  --output_dir ./tmp/trt_engines/1-gpu-int8-ckpt/ \\\n",
    "  --dtype float16 \\\n",
    "  --int8_kv_cache\n",
    "\n",
    "!trtllm-build --checkpoint_dir ./tmp/trt_engines/1-gpu-int8-ckpt/ \\\n",
    "              --output_dir ./tmp/trt_engines/llama_int8_kv_cache_only \\\n",
    "              --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr12WVu5Gah7"
   },
   "source": [
    "## 5. Optimisation Scenario 2: INT8 KV Cache + INT8 Weight-Only Quantisation (W8A16)\n",
    "\n",
    "Now, I combine **INT8 KV-cache** with **INT8 weight-only quantisation**.  \n",
    "This further reduces model size while keeping activations in FP16.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./tmp/trt_engines/1-gpu-int8-kv-wq-ckpt\n",
    "\n",
    "!python ./TensorRT-LLM/examples/models/core/llama/convert_checkpoint.py \\\n",
    "  --model_dir ./tmp/hf_models/meta-llama/Llama-3.2-1B \\\n",
    "  --output_dir ./tmp/trt_engines/1-gpu-int8-kv-wq-ckpt/ \\\n",
    "  --dtype float16 \\\n",
    "  --int8_kv_cache \\\n",
    "  --use_weight_only \\\n",
    "  --weight_only_precision int8\n",
    "\n",
    "!trtllm-build --checkpoint_dir ./tmp/trt_engines/1-gpu-int8-kv-wq-ckpt/ \\\n",
    "              --output_dir ./tmp/trt_engines/llama_int8_kv_cache_int8_wq \\\n",
    "              --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rctI1RNyGcmo"
   },
   "source": [
    "## 6. Optimisation Scenario 3: INT8 KV Cache + AWQ (W4A16, Group-wise)\n",
    "\n",
    "Finally, I apply **Activation-Aware Quantisation (AWQ)**:  \n",
    "- Weights: INT4 (group-wise, block size = 128)  \n",
    "- KV-cache: INT8  \n",
    "- Activations: FP16  \n",
    "\n",
    "This is the most aggressive compression tested here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./tmp/trt_engines/1-gpu-int8-kv-awq-ckpt\n",
    "\n",
    "!python ./TensorRT-LLM/examples/quantization/quantize.py \\\n",
    "  --model_dir ./tmp/hf_models/meta-llama/Llama-3.2-1B \\\n",
    "  --output_dir ./tmp/trt_engines/1-gpu-int8-kv-awq-ckpt/ \\\n",
    "  --dtype float16 \\\n",
    "  --qformat int4_awq \\\n",
    "  --awq_block_size 128 \\\n",
    "  --kv_cache_dtype int8 \\\n",
    "  --calib_size 32\n",
    "\n",
    "!trtllm-build --checkpoint_dir ./tmp/trt_engines/1-gpu-int8-kv-awq-ckpt/ \\\n",
    "              --output_dir ./tmp/trt_engines/llama_int8_kv_cache_int4_awq \\\n",
    "              --gemm_plugin auto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS-rxq1HGnAR"
   },
   "source": [
    "## 7. Evaluation: Short vs Long Context\n",
    "\n",
    "I now benchmark across four backends and two settings:\n",
    "\n",
    "1. **Short context (128 prompt tokens, 128 output tokens)**  \n",
    "2. **Long context (real document text, ~2000 prompt tokens, 512 output tokens)**  \n",
    "\n",
    "This lets me show how TensorRT-LLM optimisations (FP16, INT8 KV, W8A16, AWQ) scale from short inputs to long-context workloads where **KV cache quantisation is most beneficial**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorrt_llm.runtime import ModelRunner\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\"\n",
    "max_new_short = 128\n",
    "max_new_long = 512\n",
    "\n",
    "# Benchmark helpers\n",
    "def bench_pt(prompt, max_new, iters=3):\n",
    "    lat = []\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(iters):\n",
    "            t0 = time.time()\n",
    "            _ = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(device),\n",
    "                               max_new_tokens=max_new)\n",
    "            torch.cuda.synchronize()\n",
    "            lat.append((time.time()-t0)*1000)\n",
    "    ms = float(np.mean(lat))\n",
    "    tps = max_new / (ms/1000.0)\n",
    "    return ms, tps\n",
    "\n",
    "def bench_trt(runner, prompt, max_new, iters=3):\n",
    "    lat = []\n",
    "    for _ in range(iters):\n",
    "        t0 = time.time()\n",
    "        _ = runner.generate(tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")[\"input_ids\"],\n",
    "                            max_new_tokens=max_new)\n",
    "        torch.cuda.synchronize()\n",
    "        lat.append((time.time()-t0)*1000)\n",
    "    ms = float(np.mean(lat))\n",
    "    tps = max_new / (ms/1000.0)\n",
    "    return ms, tps\n",
    "\n",
    "# Try to load runners if engine dirs exist\n",
    "available_runners = {}\n",
    "\n",
    "def try_load(name, path):\n",
    "    import os\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            r = ModelRunner.from_dir(engine_dir=path, rank=0)\n",
    "            available_runners[name] = r\n",
    "            print(f\"Loaded {name} from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {name}: {e}\")\n",
    "\n",
    "# Add your engine paths here\n",
    "try_load(\"TensorRT-LLM FP16\", str(engine_fp16))\n",
    "try_load(\"TensorRT-LLM INT8 KV\", \"./tmp/trt_engines/llama_int8_kv_cache_only\")\n",
    "try_load(\"TensorRT-LLM INT8 KV + W8A16\", \"./tmp/trt_engines/llama_int8_kv_cache_int8_wq\")\n",
    "try_load(\"TensorRT-LLM INT8 KV + AWQ\", \"./tmp/trt_engines/llama_int8_kv_cache_int4_awq\")\n",
    "\n",
    "# Prepare prompts\n",
    "short_prompt = \"Summarize the benefits of KV-cache in 3 concise bullet points.\"\n",
    "gutenberg = load_dataset(\"gutenberg\", \"shakespeare-macbeth\", split=\"train\")\n",
    "long_prompt = gutenberg[0][\"text\"][:4000]  # ~2000 tokens\n",
    "print(\"Sample long prompt:\\n\", long_prompt[:300], \"...\\n\")\n",
    "\n",
    "# Run benchmarks\n",
    "def run_benchmarks(prompt, max_new, label):\n",
    "    results = []\n",
    "    # PyTorch baseline\n",
    "    pt_ms, pt_tps = bench_pt(prompt, max_new)\n",
    "    results.append({\"backend\":\"PyTorch FP16\", \"avg_ms\":pt_ms, \"tokens_per_sec\":pt_tps})\n",
    "    # TensorRT engines\n",
    "    for name, runner in available_runners.items():\n",
    "        ms, tps = bench_trt(runner, prompt, max_new)\n",
    "        results.append({\"backend\":name, \"avg_ms\":ms, \"tokens_per_sec\":tps})\n",
    "    df = pd.DataFrame(results)\n",
    "    display(df)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(df[\"backend\"], df[\"tokens_per_sec\"])\n",
    "    plt.ylabel(\"Throughput (tokens/s)\")\n",
    "    plt.title(f\"{label} (max_new={max_new})\")\n",
    "    plt.xticks(rotation=20, ha=\"right\")\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"### Short context benchmark\")\n",
    "df_short = run_benchmarks(short_prompt, max_new_short, \"Short Context\")\n",
    "\n",
    "print(\"### Long context benchmark\")\n",
    "df_long = run_benchmarks(long_prompt, max_new_long, \"Long Context\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
